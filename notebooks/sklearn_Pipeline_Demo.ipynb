{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import  train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, roc_auc_score,\n",
    "    classification_report, f1_score, roc_auc_score, recall_score, roc_curve\n",
    "    )\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 500)\n",
    "np.set_printoptions(\n",
    "    edgeitems=5, linewidth=200, suppress=True, nanstr='NaN',\n",
    "    infstr='Inf', precision=5\n",
    "    )\n",
    "\n",
    "RANDOM_STATE = 516\n",
    "TRAIN_SIZE   = .70\n",
    "TEST_SIZE    = 1 - TRAIN_SIZE\n",
    "\n",
    "\n",
    "# fpath = \"S:\\\\public\\\\Actuarial\\\\DSSG\\\\20170721_Materials\\\\mw.data\"\n",
    "# hdrs  = [\"ID\", \"GENDER\", \"HEIGHT\", \"HAND_LENGTH\", \"FOREARM_LENGTH\"]\n",
    "# df    = pd.read_table(fpath, sep=\"\\s+\", names=hdrs)\n",
    "\n",
    "\n",
    "#data_path = \"https://gist.githubusercontent.com/jtrive84/c1e23acb2624733ada178260cc3c683c/raw/3cf7e81263de12727ac4d6b7391bf6766f942f2b/admissions.csv\"\n",
    "#data_path = \"G:/Datasets/Binary_Response/Gold/gold.data\"\n",
    "# dfall = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "# categorical_vars = [\"rank\"]\n",
    "# continuous_vars  = [\"gre\", \"gpa\",]\n",
    "# response         = \"admit\"\n",
    "\n",
    "\n",
    "# Split data into train, validation and test cohorts. \n",
    "# dftrain, dftest0, ytrain, ytest0 = \\\n",
    "#     train_test_split(\n",
    "#         dfall[categorical_vars + continuous_vars], \n",
    "#         dfall[response], test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "#         )\n",
    "\n",
    "# # Split dftest0 into validation and test cohorts.\n",
    "# dfvalidate, dftest, yvalidate, ytest = \\\n",
    "#     train_test_split(\n",
    "#         dftest0, ytest0, test_size=.30, random_state=RANDOM_STATE\n",
    "#         )\n",
    "\n",
    "\n",
    "# # Recombine dftrain + ytrain, dfvalidate + yvalidate and dftest + ytest.\n",
    "# dftrain = dftrain.join(ytrain).reset_index(drop=True)\n",
    "# dfvalidate = dfvalidate.join(yvalidate).reset_index(drop=True)\n",
    "# dftest = dftest.join(ytest).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using ColumnTransformer.\n",
    "\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "    - https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "    \n",
    "Applies transformers to columns of an array or pandas DataFrame.\n",
    "\n",
    "\n",
    "NOTES:\n",
    "As implemented here, I used ColumnTransformer to pre-process the test and train data, while fitting\n",
    "the model outside of the pipeline. This allows for calling `fit_transform` on the data,\n",
    "as opposed to just `fit` as is typically used in examples that use Pipeline with an embedded \n",
    "estimator. This seems like a cleaner workflow to me, since typical workflows I've implemented\n",
    "pre-process the data once, then fit a number of separate classifiers. \n",
    "\n",
    "\n",
    "Questions:\n",
    "\n",
    "For pipelines with an embedded predictor, does `fit` do the same as `fit_transform`? Do we only\n",
    "call `fit` on these pipelines (with embedded classifiers) since classifiers do not expose a \n",
    "`fit_transform` method?\n",
    "\n",
    "Answer: Each object in the pipeline is assumed to have both fit and transform methods.\n",
    "Therefore, they are both called on each transformer.\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, roc_auc_score,\n",
    "    classification_report, f1_score, roc_auc_score, recall_score\n",
    "    )\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 500)\n",
    "np.set_printoptions(\n",
    "    edgeitems=5, linewidth=200, suppress=True, nanstr='NaN',\n",
    "    infstr='Inf', precision=5\n",
    "    )\n",
    "\n",
    "RANDOM_STATE = 516\n",
    "TRAIN_SIZE   = .70\n",
    "TEST_SIZE    = 1 - TRAIN_SIZE\n",
    "\n",
    "\n",
    "# data_path = \"G:\\\\Datasets\\\\Binary_Response\\\\Titanic\\\\Titanic.csv\"\n",
    "# dfinit = pd.read_csv(data_path)\n",
    "\n",
    "data_path = \"G:\\\\Datasets\\\\Books\\\\Wiley_Applied_Logistic_Regression\\\\NHANES.txt\"\n",
    "dfinit = pd.read_csv(data_path, sep=\"\\t\")\n",
    "\n",
    "continuous_features  = [\n",
    "    \"AGE\", \n",
    "    ]\n",
    "\n",
    "categorical_features = [\n",
    "    \"GENDER\", \"CLASS\",\n",
    "    ]\n",
    "\n",
    "response = \"OBESE\"\n",
    "\n",
    "# Convert binary indicators to string fields so we can perform \n",
    "# our own onehot encoding and maintain the base levels of each.\n",
    "\n",
    "# Redefine MARSTAT as currently married or not. \n",
    "dfinit[\"MARSTAT\"] = dfinit[\"MARSTAT\"].map(lambda v: 1 if v==1 else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.iterrows():\n",
    "    \n",
    "    if row[\"VIGRECEXR\"] == row[\"OBESE\"]:\n",
    "        \n",
    "        print(\"same\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VIGRECEXR  MODRECEXR  SEDMIN  OBESE\n",
      "0        1.0        1.0   480.0    0.0\n",
      "1        1.0        0.0   240.0    0.0\n",
      "2        1.0        1.0   240.0    1.0\n",
      "3        0.0        1.0   720.0    0.0\n",
      "4        1.0        1.0   240.0    0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[int, pandas.core.series.Series]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfinit[\"Label\"] = dfinit[\"MARSTAT\"]\n",
    "df = dfinit.iloc[:5,-5:-1]\n",
    "print(df)\n",
    "\n",
    "dflist = [r for r in df.iterrows()]\n",
    "[type(i) for i in dflist[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIGRECEXR</th>\n",
       "      <th>MODRECEXR</th>\n",
       "      <th>SEDMIN</th>\n",
       "      <th>OBESE</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VIGRECEXR  MODRECEXR  SEDMIN  OBESE  Label\n",
       "0        1.0        1.0   480.0    0.0      1\n",
       "1        1.0        0.0   240.0    0.0      0\n",
       "2        1.0        1.0   240.0    1.0      0\n",
       "3        0.0        1.0   720.0    0.0      1\n",
       "4        1.0        1.0   240.0    0.0      0\n",
       "5        1.0        0.0    60.0    0.0      1\n",
       "6        1.0        1.0   540.0    0.0      0\n",
       "7        0.0        0.0   480.0    0.0      0\n",
       "8        1.0        1.0    30.0    1.0      1\n",
       "9        1.0        1.0     NaN    0.0      1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [row[1] for row in df.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, VIGRECEXR      1.0\n",
       " MODRECEXR      0.0\n",
       " SEDMIN       240.0\n",
       " OBESE          0.0\n",
       " Label          0.0\n",
       " Name: 1, dtype: float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.iterrows())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIGRECEXR      1.0\n",
      "MODRECEXR      1.0\n",
      "SEDMIN       480.0\n",
      "OBESE          0.0\n",
      "Label          1.0\n",
      "Name: 0, dtype: float64\n",
      "VIGRECEXR      1.0\n",
      "MODRECEXR      0.0\n",
      "SEDMIN       240.0\n",
      "OBESE          0.0\n",
      "Label          0.0\n",
      "Name: 1, dtype: float64\n",
      "VIGRECEXR      1.0\n",
      "MODRECEXR      1.0\n",
      "SEDMIN       240.0\n",
      "OBESE          1.0\n",
      "Label          0.0\n",
      "Name: 2, dtype: float64\n",
      "VIGRECEXR      0.0\n",
      "MODRECEXR      1.0\n",
      "SEDMIN       720.0\n",
      "OBESE          0.0\n",
      "Label          1.0\n",
      "Name: 3, dtype: float64\n",
      "VIGRECEXR      1.0\n",
      "MODRECEXR      1.0\n",
      "SEDMIN       240.0\n",
      "OBESE          0.0\n",
      "Label          0.0\n",
      "Name: 4, dtype: float64\n",
      "VIGRECEXR     1.0\n",
      "MODRECEXR     0.0\n",
      "SEDMIN       60.0\n",
      "OBESE         0.0\n",
      "Label         1.0\n",
      "Name: 5, dtype: float64\n",
      "VIGRECEXR      1.0\n",
      "MODRECEXR      1.0\n",
      "SEDMIN       540.0\n",
      "OBESE          0.0\n",
      "Label          0.0\n",
      "Name: 6, dtype: float64\n",
      "VIGRECEXR      0.0\n",
      "MODRECEXR      0.0\n",
      "SEDMIN       480.0\n",
      "OBESE          0.0\n",
      "Label          0.0\n",
      "Name: 7, dtype: float64\n",
      "VIGRECEXR     1.0\n",
      "MODRECEXR     1.0\n",
      "SEDMIN       30.0\n",
      "OBESE         1.0\n",
      "Label         1.0\n",
      "Name: 8, dtype: float64\n",
      "VIGRECEXR    1.0\n",
      "MODRECEXR    1.0\n",
      "SEDMIN       NaN\n",
      "OBESE        0.0\n",
      "Label        1.0\n",
      "Name: 9, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for row in l:\n",
    "    \n",
    "    row = row[1]\n",
    "    print(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinit.columns = [i.upper() for i in dfinit.columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# If necessary, convert response to 0/1 with LabelBinarizer.\n",
    "# lb = LabelBinarizer()\n",
    "# df[response] = lb.fit_transform(df[response].values)\n",
    "\n",
    "\n",
    "X = pd.get_dummies(\n",
    "    dfinit[continuous_features + categorical_features],\n",
    "    columns=categorical_features, drop_first=True\n",
    "    )\n",
    "\n",
    "\n",
    "y = dfinit[response].values\n",
    "\n",
    "\n",
    "# Update categorical features to account for one hot encoding.\n",
    "categorical_features = [i for i in X.columns if i not in continuous_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup pre-processing pipeline. The implementation in this cell only handles\n",
    "pre-processing and does not fit an estimator at the end. \n",
    "\"\"\"\n",
    "# Split data into train-test cohorts.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30)\n",
    "\n",
    "continuous_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(missing_values=np.NaN, strategy=\"median\", fill_value=0)),\n",
    "    (\"scaler\" , StandardScaler())\n",
    "    ])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(missing_values=np.NaN, strategy=\"constant\", fill_value=0))\n",
    "    ])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"continuous\" , continuous_transformer, continuous_features), \n",
    "    (\"categorical\", categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "pp = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor), \n",
    "    ])\n",
    "\n",
    "# Pass training data into preprocessing pipeline.\n",
    "X_train = pp.fit(X_train)\n",
    "\n",
    "# X_train can now be used to fit any number of estimators...\n",
    "\n",
    "# lr = LogisticRegression()\n",
    "# mdl1 = lr.fit(X_train)\n",
    "# y_pred1 = mdl1.predict(X_test)\n",
    "\n",
    "# clf = RandomForestClassifier()\n",
    "# mdl2 = clf.fit(X_train)\n",
    "# y_pred2 = mdl2.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Performing grid search over specified parameter space, with estimator \n",
    "included as final step in pipeline.\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30)\n",
    "\n",
    "continuous_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(missing_values=np.NaN, strategy=\"median\", fill_value=0)),\n",
    "    (\"scaler\" , StandardScaler())\n",
    "    ])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(missing_values=np.NaN, strategy=\"constant\", fill_value=0))\n",
    "    ])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"continuous\" , continuous_transformer, continuous_features), \n",
    "    (\"categorical\", categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "lr = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor), \n",
    "    (\"classifier\"  , LogisticRegression(random_state=RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "param_grid = [{\n",
    "    \"classifier__fit_intercept\":[True, False],\n",
    "    \"classifier__penalty\"      :[\"elasticnet\"],\n",
    "    \"classifier__solver\"       :[\"saga\"],\n",
    "    \"classifier__C\"            :[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"classifier__l1_ratio\"     :np.arange(0, 1.10, .10),\n",
    "    }]\n",
    "\n",
    "gs = GridSearchCV(lr, param_grid, scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = gs.best_params_\n",
    "model     = gs.best_estimator_\n",
    "y_pred    = model.predict(X_test)\n",
    "y_prob    = model.predict_proba(X_test)[:,-1]\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob,  pos_label=1)\n",
    "# fpr2, tpr2, thresholds2 = roc_curve(y_test, lr_p_hat,  pos_label=1)\n",
    "# fpr3, tpr3, thresholds3 = roc_curve(y_test, nb_p_hat,  pos_label=1)\n",
    "# fpr4, tpr4, thresholds4 = roc_curve(y_test, knn_p_hat, pos_label=1)\n",
    "\n",
    "\n",
    "# plt.plot(fpr1, tpr1, linewidth=2, label='Random Forest')\n",
    "# plt.plot(fpr2, tpr2, linewidth=2, label='Logistic Regression')\n",
    "# plt.plot(fpr3, tpr3, linewidth=2, label='Naive Bayes')\n",
    "# plt.plot(fpr4, tpr4, linewidth=2, label='kNN')\n",
    "# plt.plot([0,1], [0,1], '--')\n",
    "# plt.axis([-.05, 1.05, -.05, 1.05])\n",
    "# plt.xlabel('False Positive Rate (FPR)')\n",
    "# plt.ylabel('True Positive Rate (TPR)')\n",
    "# plt.grid(True)\n",
    "# plt.legend(loc='lower right',prop={'size':20}, frameon=True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression model on training data.\n",
    "lr =  LogisticRegression(C=1.0, solver=\"lbfgs\")\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Preprocess test data (only call transform on test data cohort).\n",
    "X_test  = pp.transform(X_test)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_prob = lr.predict_proba(X_test)\n",
    "\n",
    "print(\"Accuracy : {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, y_pred, average=\"weighted\")))\n",
    "print(\"Recall   : {}\".format(recall_score(y_test, y_pred, average=\"weighted\")))\n",
    "print(\"f1-score : {}\".format(f1_score(y_test, y_pred, average=\"weighted\")))\n",
    "print(\"ROC-AUC  : {}\".format(roc_auc_score(y_test, y_pred)))\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using BaseEstimator + TransformerMixin.\n",
    "\n",
    "    - https://gist.github.com/amberjrivera/8c5c145516f5a2e894681e16a8095b5c\n",
    "    \n",
    "    - https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html#sphx-glr-auto-examples-compose-plot-column-transformer-py\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AverageWordLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def average_word_length(self, name):\n",
    "        \"\"\"Helper code to compute average word length of a name\"\"\"\n",
    "        return np.mean([len(word) for word in name.split()])\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        return df['road_name'].apply(self.average_word_length)\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "# Unless you’re doing something more complicated where something different happens \n",
    "# in the training and testing phase (like when extracting n-grams), this is the \n",
    "# general pattern for a transformer:\n",
    "class SampleExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, vars):\n",
    "        self.vars = vars  # e.g. pass in a column name to extract\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return do_something_to(X, self.vars)  # where the actual feature extraction happens\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # generally does nothing\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearchCV within Pipeline\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV\n",
    "\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using FeatureUnion.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/compose.html?highlight=featureunion#featureunion-composite-feature-spaces\n",
    "\n",
    "\n",
    "# We have a slight problem: we only know how to add transformers in series, but what we need \n",
    "# to do is to add our average word length transformer in parallel with the n-gram extractor.\n",
    "# For this, there is scikit-learn’s FeatureUnion class.\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram', ngram_count_pipeline), # can pass in either a pipeline\n",
    "        ('ave', AverageWordLengthExtractor()) # or a transformer\n",
    "    ])),\n",
    "    ('clf', LinearSVC())  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "# If you’re looking to do hyperparameter tuning (which I won’t explain here), pipelines \n",
    "# make that easy, as below:\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "pg = {'clf__C': [0.1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=pg, cv=5)\n",
    "grid.fit(data_train, y_train)\n",
    "\n",
    "grid.best_params_\n",
    "# {'clf__C': 0.1}\n",
    "\n",
    "grid.best_score_\n",
    "# 0.702290076336\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Pipeline and make_pipeline\n",
    "\n",
    "> https://stackoverflow.com/questions/40708077/what-is-the-difference-between-pipeline-and-make-pipeline-in-scikit\n",
    "\n",
    "\n",
    "`Pipeline`:\n",
    "\n",
    "- names are explicit, you don't have to figure them out if you need them;\n",
    "- name doesn't change if you change estimator/transformer used in a step, e.g. if you replace `LogisticRegression()` with `LinearSVC()` you can still use `clf__C`.\n",
    "\n",
    "\n",
    "`make_pipeline`:\n",
    "\n",
    "- shorter and arguably more readable notation\n",
    "- names are auto-generated using a straightforward rule (lowercase name of an estimator)\\\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with sklearn JDTAQ\n",
    "\n",
    "\n",
    "*Why do we call `fit_transform` on training data but only `transform` on test data?*\n",
    "\n",
    "The reason you want to fit the scaler using only the training data is because you don't want to bias your \n",
    "model with information from the test data.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
