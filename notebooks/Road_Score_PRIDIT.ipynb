{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Quality Scoring: An Unsupervised Approach\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### References:\n",
    "- Brocket, et. al.: [*Fraud Classification Using Pricipal Component Analysis of RIDITs*, The Journal of Risk and Insurance, 2002 Vol. 69, No. 3, 341-371.](https://www.fox.temple.edu/cms/wp-content/uploads/2012/06/Richard-Derrig_11.pdf).      \n",
    "<br>      \n",
    "- Frees, et. al.: *Predictive Modeling Applications in Actuarial Science Volume II; Case Studies in Insurance*: Cambridge University Press 2016, pp. 180-189.      \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "PRIDIT, an acronym for *Principal Components of RIDITS*, are a percentile-based statistic. The RIDIT \n",
    "transformation is generally applied to variables whose values can be considered in some sense to be \n",
    "ordered. These might be answers to a survey (i.e., disagree, neutral, agree, etc.), but the \n",
    "variables might also be binary or categorical variables believed to be related to suspicion of a \n",
    "questionable claim and the other to a likely legitimate claim.   \n",
    "\n",
    "PRIDIT analysis is comprised of two steps: The first involves calculating a RIDIT score for \n",
    "each quality measure. The second step applies a Principal Components transformation to the RIDIT \n",
    "scores. The primary assumption within the context of FWA data is that the first principal component \n",
    "is the component representing questionable activity, and any remaining variation in the data is due \n",
    "to other factors not directly related to questionable activity. Each component is mathematically \n",
    "unrelated or orthogonal, but the first component assumption is based on a combination of \n",
    "the selected feratures and expert judgment.         \n",
    "\n",
    "The first principal component is the basis for the PRIDIT scores. PRIDIT applies the factor weights to the \n",
    "normalized RIDIT scores such that each claim receives a final weighted average RIDIT score, or *PRIDIT\n",
    "score*. By default, PRIDIT scores data on a [-1, 1] scale. The final results have been rescaled to the \n",
    "[0, 100] range for ease of interpretation.\n",
    "\n",
    "The meaning of PRIDIT will change based on the type of data used to calculate the scores. The first \n",
    "principle component generated from PCA is the basis of the PRIDIT algorithm, which combines the \n",
    "variation in all features input into the system. We assume that variable categories are ordered \n",
    "in decreasing likelihood of fraud suspicion such that higher categorical responses are associated \n",
    "with a lower propensity of fraud. There is no dependent or \"left hand side\" variable to assess the \n",
    "results of PRIDIT. Instead:\n",
    "\n",
    "\n",
    "> ***The actual result of PRIDIT is simply the strongest determinant of \n",
    "> variation in the data â€” what we choose to put into the model determines what we get out of it.\n",
    "> The final PRIDIT score is a latent measure of underlying road quality.***\n",
    "\n",
    "<br>\n",
    "\n",
    "## PRIDIT Score Formulation\n",
    "\n",
    "The scoring logic is described in detail in Brocket (2002) Appendix A., but an high-level overview is provided \n",
    "here. \n",
    "\n",
    "To begin, we create a 2-dimensional array consisting of RIDIT scores. The RIDIT transformation is typically \n",
    "applied to variables with ordered levels, and is considered distribution free. If a variable does not have a \n",
    "pre-specified ordering (such as {\"Male\", \"Female\"}, {\"Red\", \"Green\", \"Blue\"}, etc.),  we can perform a \n",
    "one-hot encoding, transforming a feature with $k$ levels into $k-1$ indicator columns.          \n",
    "The RIDIT score represents a probability transformation based on the empirical distribution of the data.  \n",
    "For categorical variable $X_{i}$ with levels having known ordinal relationship, the proportion of records for each \n",
    "level is computed, along with the cumulative proportion across all levels from low to high. Continuous features \n",
    "are binned using no more than ten categories. After the cumulative proportions have been determined for all \n",
    "features, RIDIT scores are computed for each $X_{i}$ using one of two similar formulations:    \n",
    "<br>    \n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "RIDIT(X_{i}) &= P(X \\lt X_{i-1}) - P(X \\gt X_{i}) \\\\\n",
    "             &= P(X \\lt X_{i}) + \\frac{1}{2}P(X=X_{i})\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "The second formulation represents variable levels on a $[0, 1]$ scale, while the Brocket (2002) formulation \n",
    "ranges from $[-1, +1]$. \n",
    "\n",
    "<br>\n",
    "\n",
    "- $X$ = Matrix of RIDIT scores  \n",
    "<br>  \n",
    "- $B$ = Normalized RIDIT matrix   \n",
    "<br>  \n",
    "- $\\lambda$ = First eigenvalue resulting from PCA   \n",
    "<br>  \n",
    "- $\\bar{x}$ = First eigenvector resulting from PCA    \n",
    "<br>  \n",
    "- $w$ = PRIDIT weights, given by $\\sqrt{\\lambda}\\bar{x}$       \n",
    "<br>  \n",
    "\n",
    "$B$ is given by\n",
    "\n",
    "$$\n",
    "B = \\frac{X^{T}X}{\\sqrt{diag(X^{T}X)}},\n",
    "$$\n",
    "<br>   \n",
    "which represents the normalized RIDIT matrix. Once we have $B$, PRIDIT scores are computed as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{PRIDIT} = \\frac{B w}{\\lambda}.\n",
    "$$\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 500)\n",
    "np.set_printoptions(suppress=True, precision=5, linewidth=1000)\n",
    "\n",
    "DATESTAMP = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "DATA_URL = \"https://gist.githubusercontent.com/jtrive84/9b77fd73aded95ff456e652fcd48c451/raw/2151ed12da23ffa5df8e2eacb69f2ee3026dd683/Claims.csv\"\n",
    "NBR_BINS = 10\n",
    "\n",
    "dfinit = pd.read_csv(DATA_URL, delimiter=\",\", header=0)\n",
    "dfinit.columns = dfinit.columns.str.lower()\n",
    "\n",
    "response = \"suspicion\"\n",
    "\n",
    "key_columns = [\"id\"]\n",
    "\n",
    "categorical_nominal = []\n",
    "categorical_ordinal = [\n",
    "    \"legalrep\", \"sprain\", \"chiropt\", \"emtreat\", \"police\", \"prior\", \"numprov\",\n",
    "    \"inj01\", \"inj02\", \"inj06\", \"ins06\", \"acc04\", \"acc09\", \"acc10\", \"acc15\",\n",
    "    \"acc19\", \"clt07\"\n",
    "    ]\n",
    "categorical = categorical_nominal + categorical_ordinal\n",
    "continuous = [\n",
    "    \"rptlag\", \"trtlag\", \"pollag\", \"fault\", \"ambulcost\", \"hospitalbill\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing `rptlag`.\n",
      "Preprocessing `trtlag`.\n",
      "Preprocessing `pollag`.\n",
      "Preprocessing `fault`.\n",
      "Preprocessing `ambulcost`.\n",
      "Preprocessing `hospitalbill`.\n",
      "Preprocessing `legalrep`.\n",
      "Preprocessing `sprain`.\n",
      "Preprocessing `chiropt`.\n",
      "Preprocessing `emtreat`.\n",
      "Preprocessing `police`.\n",
      "Preprocessing `prior`.\n",
      "Preprocessing `numprov`.\n",
      "Preprocessing `inj01`.\n",
      "Preprocessing `inj02`.\n",
      "Preprocessing `inj06`.\n",
      "Preprocessing `ins06`.\n",
      "Preprocessing `acc04`.\n",
      "Preprocessing `acc09`.\n",
      "Preprocessing `acc10`.\n",
      "Preprocessing `acc15`.\n",
      "Preprocessing `acc19`.\n",
      "Preprocessing `clt07`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rptlag_ridit</th>\n",
       "      <th>trtlag_ridit</th>\n",
       "      <th>pollag_ridit</th>\n",
       "      <th>fault_ridit</th>\n",
       "      <th>ambulcost_ridit</th>\n",
       "      <th>hospitalbill_ridit</th>\n",
       "      <th>legalrep_ridit</th>\n",
       "      <th>sprain_ridit</th>\n",
       "      <th>chiropt_ridit</th>\n",
       "      <th>emtreat_ridit</th>\n",
       "      <th>police_ridit</th>\n",
       "      <th>prior_ridit</th>\n",
       "      <th>numprov_ridit</th>\n",
       "      <th>inj01_ridit</th>\n",
       "      <th>inj02_ridit</th>\n",
       "      <th>inj06_ridit</th>\n",
       "      <th>ins06_ridit</th>\n",
       "      <th>acc04_ridit</th>\n",
       "      <th>acc09_ridit</th>\n",
       "      <th>acc10_ridit</th>\n",
       "      <th>acc15_ridit</th>\n",
       "      <th>acc19_ridit</th>\n",
       "      <th>clt07_ridit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.287667</td>\n",
       "      <td>0.653667</td>\n",
       "      <td>0.873667</td>\n",
       "      <td>0.396333</td>\n",
       "      <td>0.317667</td>\n",
       "      <td>0.250667</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.580333</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.265667</td>\n",
       "      <td>0.277667</td>\n",
       "      <td>0.575667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.619667</td>\n",
       "      <td>0.510333</td>\n",
       "      <td>0.417333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.287667</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.396333</td>\n",
       "      <td>0.317667</td>\n",
       "      <td>0.250667</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.580333</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.672333</td>\n",
       "      <td>0.765667</td>\n",
       "      <td>0.777667</td>\n",
       "      <td>0.575667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.589333</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.619667</td>\n",
       "      <td>0.510333</td>\n",
       "      <td>0.417333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.287667</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>0.647000</td>\n",
       "      <td>0.396333</td>\n",
       "      <td>0.317667</td>\n",
       "      <td>0.750667</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.580333</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.265667</td>\n",
       "      <td>0.277667</td>\n",
       "      <td>0.575667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.589333</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.619667</td>\n",
       "      <td>0.510333</td>\n",
       "      <td>0.417333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.287667</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.396333</td>\n",
       "      <td>0.817667</td>\n",
       "      <td>0.750667</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.580333</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.265667</td>\n",
       "      <td>0.277667</td>\n",
       "      <td>0.575667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.619667</td>\n",
       "      <td>0.510333</td>\n",
       "      <td>0.417333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.287667</td>\n",
       "      <td>0.653667</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.396333</td>\n",
       "      <td>0.817667</td>\n",
       "      <td>0.750667</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.580333</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.265667</td>\n",
       "      <td>0.277667</td>\n",
       "      <td>0.075667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.062333</td>\n",
       "      <td>0.510333</td>\n",
       "      <td>0.059333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rptlag_ridit  trtlag_ridit  pollag_ridit  fault_ridit  ambulcost_ridit  hospitalbill_ridit  legalrep_ridit  sprain_ridit  chiropt_ridit  emtreat_ridit  police_ridit  prior_ridit  numprov_ridit  inj01_ridit  inj02_ridit  inj06_ridit  ins06_ridit  acc04_ridit  acc09_ridit  acc10_ridit  acc15_ridit  acc19_ridit  clt07_ridit\n",
       "0      0.458333         0.445      0.287667     0.653667         0.873667            0.396333        0.317667      0.250667           0.68       0.580333         0.746        0.422       0.277000     0.265667     0.277667     0.575667        0.533     0.089333     0.493333     0.554667     0.619667     0.510333     0.417333\n",
       "1      0.458333         0.445      0.287667     0.855000         0.243000            0.396333        0.317667      0.250667           0.68       0.580333         0.246        0.422       0.672333     0.765667     0.777667     0.575667        0.533     0.589333     0.493333     0.554667     0.619667     0.510333     0.417333\n",
       "2      0.458333         0.445      0.287667     0.763000         0.647000            0.396333        0.317667      0.750667           0.68       0.580333         0.246        0.422       0.026000     0.265667     0.277667     0.575667        0.533     0.589333     0.493333     0.166667     0.619667     0.510333     0.417333\n",
       "3      0.458333         0.927      0.287667     0.956000         0.243000            0.396333        0.817667      0.750667           0.68       0.580333         0.746        0.422       0.277000     0.265667     0.277667     0.575667        0.533     0.089333     0.493333     0.166667     0.619667     0.510333     0.417333\n",
       "4      0.458333         0.445      0.287667     0.653667         0.243000            0.396333        0.817667      0.750667           0.68       0.580333         0.746        0.922       0.026000     0.265667     0.277667     0.075667        0.533     0.089333     0.493333     0.166667     0.062333     0.510333     0.059333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute features and bin continuous features. \n",
    "\n",
    "all_features = continuous + categorical\n",
    "df = dfinit[key_columns + all_features]\n",
    "cont_imputer = SimpleImputer(missing_values=np.NaN, strategy=\"most_frequent\")\n",
    "cat_imputer = SimpleImputer(missing_values=np.NaN, strategy=\"mean\")\n",
    "\n",
    "for col_name in continuous:\n",
    "    print(f\"Preprocessing `{col_name}`.\")\n",
    "    df[col_name] = cont_imputer.fit_transform(df[col_name].values.reshape(-1, 1))\n",
    "    df[f\"{col_name}_orig\"] = df[col_name]\n",
    "    df[col_name] =   pd.cut(df[col_name], bins=NBR_BINS, labels=False)\n",
    "     \n",
    "for col_name in categorical:\n",
    "    print(f\"Preprocessing `{col_name}`.\")\n",
    "    df[col_name] = cat_imputer.fit_transform(df[col_name].values.reshape(-1, 1))\n",
    "\n",
    "# Perform RIDIT transformation for each feature.\n",
    "for var_name in all_features:\n",
    "    dfvar = df[var_name].value_counts().sort_index().reset_index(drop=False).rename({\"index\":var_name, var_name:\"n\"}, axis=1)\n",
    "    dfvar[\"n_cum\"] = dfvar[\"n\"].cumsum().shift(periods=1, fill_value=0)\n",
    "    dfvar[f\"{var_name}_ridit\"] = (.50 * dfvar[\"n\"] + dfvar[\"n_cum\"]) / df.shape[0]\n",
    "    dfvar = dfvar[[var_name, f\"{var_name}_ridit\"]]\n",
    "    df = pd.merge(df, dfvar[[var_name, f\"{var_name}_ridit\"]], on=var_name, how=\"left\")\n",
    "    \n",
    "dfridit = df.filter(regex=\"_ridit$\", axis=1)\n",
    "\n",
    "\n",
    "dfridit.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comparison of Numpy vs. sklearn PCA. \n",
    "\"\"\"\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# `pca.components_` has shape [n_components, nfeatures]. PC1 is the first \n",
    "# row of pca.components.\n",
    "\n",
    "# sklearn.\n",
    "pca = PCA(n_components=None)\n",
    "pca.fit(X)\n",
    "\n",
    "# numpy. \n",
    "U, S, Vt = linalg.svd(X, full_matrices=False)\n",
    "max_abs_columns = np.argmax(np.abs(U), axis=0)\n",
    "signs = np.sign(U[max_abs_columns, range(U.shape[1])])\n",
    "U*=signs\n",
    "Vt*=signs[:, np.newaxis]\n",
    "\n",
    "assert_array_almost_equal(Vt, pca.components_)\n",
    "\n",
    "# `pca.transform` calculates the loadings. Applies dimensionality reduction.\n",
    "# Returns a reduced dimensionality data matrix.\n",
    "\n",
    "# sklearn.\n",
    "Xpca = pca.transform(X)\n",
    "\n",
    "# numpy.\n",
    "Xpca2 = X @ Vt.T\n",
    "\n",
    "assert_array_almost_equal(Xpca, Xpca2)\n",
    "\n",
    "# `pca.inverse_transform` obtains the projection onto the components in signal \n",
    "# space. This can be used to determine reconstruction error. \n",
    "\n",
    "# sklearn.\n",
    "Xproj = pca.inverse_transform(Xpca)\n",
    "\n",
    "# numpy.\n",
    "Xproj2 = Xpca2 @ Vt + X.mean(0)\n",
    "\n",
    "assert_array_almost_equal(Xproj, Xproj2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Walkthrough of PRIDIT calculation. Final scores fall in the range of 0-100, \n",
    "with higher scores associated with claims that should be reviewed with a\n",
    "greater sense of urgency. Claims with scores between 0-50 can be discarded, \n",
    "with investigative resources primarily focused on claims with scores between \n",
    "90-100. The threshold can be adjusted over time to settle on a suitable number\n",
    "of claims flagged for review each month.\n",
    "\"\"\"\n",
    "Xinit = dfridit.values\n",
    "X_mean = Xinit.mean(axis=0)\n",
    "X = Xinit - X_mean\n",
    "\n",
    "# SVD -------\n",
    "# Adjust the columns of U and the rows of Vt such that the \n",
    "# loadings in the columns in U that are largest in absolute value are \n",
    "# always positive.\n",
    "# U: Unitary matrix (U^{T} @ U = U @ U^{T} = U @ U^{-1} = I). Orthonormal (U^{T} = U^{-1}). \n",
    "#    Columns represent eigenvectors of X @ X^{T}. \n",
    "# S: 1-D array representing singular values.\n",
    "# Vt: Unitary matrix. Orthonormal. Rows represent eigenvectors of X^T @ X.\n",
    "# Eigenvalues = S**2. \n",
    "U, S, Vt = linalg.svd(X, full_matrices=False)\n",
    "max_abs_columns = np.argmax(np.abs(U), axis=0)\n",
    "signs = np.sign(U[max_abs_columns, range(U.shape[1])])\n",
    "U*=signs\n",
    "Vt*=signs[:, np.newaxis]\n",
    "\n",
    "components = Vt\n",
    "singular_values =  S\n",
    "eigenvalues = S**2\n",
    "eigenvectors = components\n",
    "e_val_1 = eigenvalues[0]\n",
    "e_vec_1 = eigenvectors[0]\n",
    "\n",
    "# Get variance explained by singular values.\n",
    "explained_variance = eigenvalues / (X.shape[0] - 1)\n",
    "total_variance = explained_variance.sum()\n",
    "explained_variance_ratio = explained_variance / total_variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigen Decomposition -------\n",
    "R = np.cov(X, rowvar=False)\n",
    "e_vals, e_vecs = linalg.eig(R)\n",
    "\n",
    "# max_e_val_indx = np.argmax(e_vals)\n",
    "# max_e_val = e_vals[e_value_1_indx]\n",
    "\n",
    "\n",
    "# # Determine eigenvalues and eigenvectors of R. The first eigenvector is \n",
    "# # the column vector having the same columnwise index as the first eigenvalue. \n",
    "# # The first eigenvalue is the absolute maximum value found in `e_values_`. \n",
    "# # From the numpy documentation, eigenvalues are not necessarily ordered, \n",
    "# # therefore we identify the index of the absolute maximum eigenvalue and use \n",
    "# # it to reference the first eigenvector.\n",
    "# e_values_, e_vectors_ = np.linalg.eig(R)\n",
    "# e_value_1_indx = np.argwhere(e_values_==np.abs(e_values_).max()).item()\n",
    "# e_value_1 = e_values_[e_value_1_indx]\n",
    "# e_vector_1 = e_vectors_[:, [e_value_1_indx]]\n",
    "\n",
    "# # e_vector_1 = e_vectors_[:, [np.argwhere(np.abs(e_vectors_[0,:])==np.abs(e_vectors_[0,:]).max()).item()]]\n",
    "# # Verify correctness of PCA transformation.\n",
    "# # [1] The sum of the eigenvalues should equal the trace of R.\n",
    "# # [2] The determinant of R should equal the product of the eigenvalues.\n",
    "# assert np.allclose(e_values_.sum(), np.diag(R).sum())\n",
    "# assert np.allclose(np.linalg.det(R), e_values_.prod())\n",
    "\n",
    "# # Normalize Xinit and compute PRIDIT weights, w, using first eigenvector.\n",
    "# w = np.sqrt(e_value_1) * e_vector_1\n",
    "# norm_factors = np.sqrt(np.diagonal(Xinit.T @ Xinit))\n",
    "# Bnorm = Xinit / norm_factors\n",
    "# pridit_scores = (Bnorm @ w) / e_value_1\n",
    "\n",
    "# # Append scores to original DataFrame, then transform to range [0-100].\n",
    "# dfpridit = df.copy(deep=True)\n",
    "# dfpridit[\"score_init\"] = pridit_scores.ravel()\n",
    "\n",
    "# # Transform scores from [-1, +1] to [0, 100] range.\n",
    "# scores_ = dfpridit[\"score_init\"].values\n",
    "# dfpridit[\"score\"] = 100 * (\n",
    "#     (scores_ - scores_.min()) / (scores_.max() - scores_.min())\n",
    "#     )\n",
    "\n",
    "# dfpridit[keycols + [\"score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02659,  0.02528, -0.01624, -0.00122, -0.21379,  0.03822,  0.32085,  0.38651,  0.31313,  0.1615 ,  0.23675,  0.14472, -0.36034, -0.37271, -0.36346, -0.10933, -0.04215, -0.14911, -0.01229, -0.20908, -0.09621, -0.01171, -0.08757])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whitening transformation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put RIDIT matrix in standard score format (s.t. columns have unit variance).\n",
    "Z = (Xinit - Xinit.mean(axis=0)) / Xinit.std(axis=0) \n",
    "# Verify columnwise mean and variance equals 0 and 1 respectively. \n",
    "assert np.allclose(np.sum(Z.var(axis=0) - 1), 0)\n",
    "assert np.allclose(np.sum(Z.mean(axis=0)), 0)\n",
    "\n",
    "# Compute R, correlation matrix of Z. Create dfcorr for export. \n",
    "dfcorr = pd.DataFrame(Z).corr()\n",
    "R = dfcorr.values\n",
    "\n",
    "# Determine eigenvalues and eigenvectors of R. The first eigenvector is \n",
    "# the column vector having the same columnwise index as the first eigenvalue. \n",
    "# The first eigenvalue is the absolute maximum value found in `e_values_`. \n",
    "# From the numpy documentation, eigenvalues are not necessarily ordered, \n",
    "# therefore we identify the index of the absolute maximum eigenvalue and use \n",
    "# it to reference the first eigenvector.\n",
    "e_values_, e_vectors_ = np.linalg.eig(R)\n",
    "e_value_1_indx = np.argwhere(e_values_==np.abs(e_values_).max()).item()\n",
    "e_value_1 = e_values_[e_value_1_indx]\n",
    "e_vector_1 = e_vectors_[:, [e_value_1_indx]]\n",
    "\n",
    "# e_vector_1 = e_vectors_[:, [np.argwhere(np.abs(e_vectors_[0,:])==np.abs(e_vectors_[0,:]).max()).item()]]\n",
    "# Verify correctness of PCA transformation.\n",
    "# [1] The sum of the eigenvalues should equal the trace of R.\n",
    "# [2] The determinant of R should equal the product of the eigenvalues.\n",
    "assert np.allclose(e_values_.sum(), np.diag(R).sum())\n",
    "assert np.allclose(np.linalg.det(R), e_values_.prod())\n",
    "\n",
    "# Normalize Xinit and compute PRIDIT weights, w, using first eigenvector.\n",
    "w = np.sqrt(e_value_1) * e_vector_1\n",
    "norm_factors = np.sqrt(np.diagonal(Xinit.T @ Xinit))\n",
    "Bnorm = Xinit / norm_factors\n",
    "pridit_scores = (Bnorm @ w) / e_value_1\n",
    "\n",
    "# Append scores to original DataFrame, then transform to range [0-100].\n",
    "dfpridit = df.copy(deep=True)\n",
    "dfpridit[\"score_init\"] = pridit_scores.ravel()\n",
    "\n",
    "# Transform scores from [-1, +1] to [0, 100] range.\n",
    "scores_ = dfpridit[\"score_init\"].values\n",
    "dfpridit[\"score\"] = 100 * (\n",
    "    (scores_ - scores_.min()) / (scores_.max() - scores_.min())\n",
    "    )\n",
    "\n",
    "dfpridit[keycols + [\"score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Consolidate variable levels and impute missing values. For categorical \n",
    "features, use strategy=\"most_frequent\", for continuous variates, use \n",
    "strategy=\"mean\".\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Convert BENEFIT_PERIOD to Lifetime vs. Non-Lifetime [\"Lifetime\", \"Non-Lifetime\",]\n",
    "print(\"Grouping levels for `BENEFIT_PERIOD`\")\n",
    "df = df.rename({\"BENEFIT_PERIOD\":\"BENEFIT_PERIOD_ORIG\"}, axis=1)\n",
    "df[\"BENEFIT_PERIOD\"] = df[\"BENEFIT_PERIOD_ORIG\"].map(lambda v: \"Lifetime\" if v==\"Lifetime\" else \"Non-Lifetime\")\n",
    "\n",
    "# Transform REPEATED_CALLS into 0/1 indicator field.\n",
    "print(\"Grouping levels for `REPEATED_CALLS`\")\n",
    "df[\"REPEATED_CALLS_IND\"] = df[\"MAX_REPEATED_CALLS\"].map(lambda v: 0 if v==0 else 1)\n",
    "df = df.drop([\"MAX_REPEATED_CALLS\",], axis=1)\n",
    "\n",
    "# Impute missing values.\n",
    "excl_vars_ = ['CLAIM_NUMBER', 'POLICY_NUMBER', 'FRAUD_INDICATOR']\n",
    "incl_vars_ = [var_ for var_ in df.columns.tolist() if var_ not in excl_vars_]\n",
    "\n",
    "for var_ in incl_vars_:\n",
    "    dfiter = df[var_].value_counts()\n",
    "    if dfiter.sum()!=nrows:\n",
    "        print(\"Imputing values for `{}`\".format(var_))\n",
    "        if dfiter.size>30:\n",
    "            # var_ is continuous - strategy should be \"mean\".\n",
    "            missing_val, strategy = np.NaN, \"mean\"\n",
    "        else:\n",
    "            # var_ is categorical - strategy should be \"most_frequent\".\n",
    "            missing_val, strategy = \"\" if df[var_].isin([\"\"]).any() else np.NaN, \"most_frequent\"\n",
    "            \n",
    "        df = df.rename({var_:\"{}_ORIG\".format(var_)}, axis=1)\n",
    "        imp_var = SimpleImputer(missing_values=missing_val, strategy=strategy)\n",
    "        df[var_] = imp_var.fit_transform(df[\"{}_ORIG\".format(var_)].values.reshape(-1, 1)).ravel()\n",
    "\n",
    "        \n",
    "# Specify group bounds for continuous variates.\n",
    "q1 = np.arange(0, 1.1, .1)\n",
    "q2 = np.roll(q1, -1)\n",
    "q1 = np.round(q1[:-1], 5)\n",
    "q2 = np.round(q2[:-1], 5)\n",
    "vv = list(enumerate(zip(q1, q2)))\n",
    "\n",
    "grpvars = []\n",
    "if len(cont_vars)>0:\n",
    "    for var_ in cont_vars:\n",
    "        print(\"Encoding values for `{}`\".format(var_))\n",
    "        grpvar_ = \"{}_GROUP\".format(var_)\n",
    "        sclvar_ = \"{}_SCALED\".format(var_)\n",
    "        df[sclvar_] = (df[var_].values - df[var_].values.min()) / (df[var_].values.max() - df[var_].values.min())\n",
    "        df[grpvar_] = -1\n",
    "        for grp_, bound_ in vv:\n",
    "            l_, u_ = bound_\n",
    "            df[grpvar_][(df[sclvar_]>=l_) & (df[sclvar_]<u_)] = grp_\n",
    "        df[grpvar_][df[sclvar_]==1.0] = df[grpvar_].max()\n",
    "\n",
    "        grpvars.append(grpvar_)   \n",
    "    df[grpvar_] = df[grpvar_].astype(np.int_)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for the RIDIT analysis, variable levels must be ordered in decreasing likelihood of fraud suspicion such that a higher categorical response indicates a lesser suspicion of fraud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Order variable levels for RIDIT analysis. Ranking specifications are available in \n",
    "the FWA repository here:\n",
    "\n",
    "    - Documents/FWA_Model_2_Variable_Selection_Variable_Ordering_20190819.xlsx\n",
    "    \n",
    "\"\"\"\n",
    "dvorder = {\n",
    "    \"ATTAINED_AGE_BANDED\"          :['50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80-84', '85-89', '>=90',],\n",
    "    \"BENEFIT_PERIOD\"               :[\"Lifetime\", \"Non-Lifetime\",], \n",
    "    \"BENEFIT_TRIGGER_OPTIONS\"      :['Medical Necessity', 'Triple Trigger', 'ADL/Cognitive',],\n",
    "    \"COLI\"                         :['Y', 'N',],\n",
    "    \"DAILY_BENEFIT_INFL_BANDED\"    :['>=$500', '>=$400 & <$500', '>=$300 & <$400', '>=$200 & <$300', '>=$100 & <$200', '<$100',],\n",
    "    \"DUAL_WAIVER\"                  :['Y', 'N',],\n",
    "    \"INDEMNITY_VS_EXPENSE_INCURRED\":['Indemnity', 'Expense Incurred',],\n",
    "    \"LINKED_POLICY_INDICATOR\"      :[1, 0,], \n",
    "    \"REPEATED_CALLS_IND\"           :[1, 0,],\n",
    "    \"PREMIUM_PAYMENT_MODE\"         :['Annual', 'Semi-Annual', 'Quarterly', 'Monthly',], \n",
    "    \"PREMIUM_WAIVED\"               :['Y', 'N',],\n",
    "    \"RESTORATION_OF_BENEFITS\"      :['Y', 'N',],\n",
    "    \"SITUS_CURRENT\"                :['HC', 'ALF', 'NH',],\n",
    "    \"TAX_QUALIFIED_STATUS\"         :['NTQ', 'TQ',],\n",
    "    \"UNDERWRITING_CLASS\"           :['Substandard', 'Standard', 'Preferred',],   \n",
    "    \"ELIM_PERIOD_BANDED\"           :['0', '>0 & <=30', '60', '90', '>90',],\n",
    "    }\n",
    "\n",
    "if len(grpvars)>0:\n",
    "    contorder = {\n",
    "        v_:df[v_].value_counts().sort_index(ascending=False).index.tolist()\n",
    "            for v_ in grpvars\n",
    "        }\n",
    "\n",
    "    dvorder.update(contorder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execute RIDIT transformation for each variable, then examine first \n",
    "few records of dfridit.\n",
    "\"\"\"\n",
    "dfridit = pd.DataFrame(columns=df.columns, index=df.index)\n",
    "\n",
    "for var_ in dvorder:\n",
    "    # var_ = \"REPEATED_CALLS\" \n",
    "    timestamp_ = datetime.datetime.now().strftime(\"%c\")\n",
    "    print(\"[{}] Computing RIDIT transformation for `{}`\".format(timestamp_, var_))\n",
    "    \n",
    "    dfvar_ = df[var_].value_counts().reset_index(drop=False).rename({\"index\":var_, var_:\"n\"}, axis=1)\n",
    "    var_order_init_ = dvorder[var_]\n",
    "    \n",
    "    # Only retain variable levels present in target dataset.\n",
    "    var_order_retain_ = [i for i in var_order_init_ if i in dfvar_[var_].values]\n",
    "    var_indx_, var_order_ = zip(*enumerate(var_order_retain_))\n",
    "    \n",
    "    # Create lookup dataframe for variable level ordering.\n",
    "    dflkp_ = pd.DataFrame({var_:var_order_, \"rank\":var_indx_})\n",
    "    \n",
    "    # Merge dflkp_ with dfvar_ then sort records by specified rank.\n",
    "    dfvar_ = pd.merge(dfvar_, dflkp_, on=var_, how=\"left\").sort_values(\"rank\").reset_index(drop=True)\n",
    "\n",
    "    # Compute cumulative sum of N on ordered dfvar_ then shift results forward 1 period.\n",
    "    dfvar_[\"n_cum\"] = dfvar_[\"n\"].cumsum().shift(periods=1, fill_value=0)\n",
    "\n",
    "    # Compute unscaled and scaled RIDIT scores.\n",
    "    dfvar_[\"ridit\"] = (.50 * dfvar_[\"n\"].values + dfvar_[\"n_cum\"].values) / nrows\n",
    "    dfvar_[\"ridit_scaled\"] = (2 * dfvar_[\"ridit\"]) - 1\n",
    "    varridit_ = pd.merge(df[[var_]], dfvar_[[var_, \"ridit_scaled\"]], on=var_, how=\"left\")\n",
    "    dfridit[var_] = varridit_[\"ridit_scaled\"]\n",
    "    \n",
    "    \n",
    "dfridit = dfridit[[i for i in dfridit.columns if not i.endswith(\"_ORIG\",)]]\n",
    "for col_ in keycols: \n",
    "    dfridit[col_] = df[col_]\n",
    "    \n",
    "# Reorder columns to place key fields at far left.\n",
    "excl_vars_ = [i.replace(\"_GROUP\", \"\") for i in grpvars] + [i.replace(\"_GROUP\", \"_SCALED\") for i in grpvars]\n",
    "varcols = sorted([i for i in dfridit.columns if i not in keycols + excl_vars_])\n",
    "colorder = keycols + varcols\n",
    "dfridit = dfridit[colorder]\n",
    "dfridit.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Walkthrough of PRIDIT calculation. Final scores fall in the range of 0-100, \n",
    "with higher scores associated with claims that should be reviewed with a\n",
    "greater sense of urgency. Claims with scores between 0-50 can be discarded, \n",
    "with investigative resources primarily focused on claims with scores between \n",
    "90-100. The threshold can be adjusted over time to settle on a suitable number\n",
    "of claims flagged for review each month.\n",
    "\"\"\"\n",
    "dfvars = dfridit[varcols]\n",
    "Xinit = dfvars.values\n",
    "\n",
    "# Put RIDIT matrix in standard score format (s.t. columns have unit variance).\n",
    "Z = (Xinit - Xinit.mean(axis=0)) / Xinit.std(axis=0) \n",
    "# Verify columnwise mean and variance equals 0 and 1 respectively. \n",
    "assert np.allclose(np.sum(Z.var(axis=0) - 1), 0)\n",
    "assert np.allclose(np.sum(Z.mean(axis=0)), 0)\n",
    "\n",
    "# Compute R, correlation matrix of Z. Create dfcorr for export. \n",
    "dfcorr = pd.DataFrame(Z).corr()\n",
    "R = dfcorr.values\n",
    "\n",
    "# Determine eigenvalues and eigenvectors of R. The first eigenvector is \n",
    "# the column vector having the same columnwise index as the first eigenvalue. \n",
    "# The first eigenvalue is the absolute maximum value found in `e_values_`. \n",
    "# From the numpy documentation, eigenvalues are not necessarily ordered, \n",
    "# therefore we identify the index of the absolute maximum eigenvalue and use \n",
    "# it to reference the first eigenvector.\n",
    "e_values_, e_vectors_ = np.linalg.eig(R)\n",
    "e_value_1_indx = np.argwhere(e_values_==np.abs(e_values_).max()).item()\n",
    "e_value_1 = e_values_[e_value_1_indx]\n",
    "e_vector_1 = e_vectors_[:, [e_value_1_indx]]\n",
    "\n",
    "# e_vector_1 = e_vectors_[:, [np.argwhere(np.abs(e_vectors_[0,:])==np.abs(e_vectors_[0,:]).max()).item()]]\n",
    "# Verify correctness of PCA transformation.\n",
    "# [1] The sum of the eigenvalues should equal the trace of R.\n",
    "# [2] The determinant of R should equal the product of the eigenvalues.\n",
    "assert np.allclose(e_values_.sum(), np.diag(R).sum())\n",
    "assert np.allclose(np.linalg.det(R), e_values_.prod())\n",
    "\n",
    "# Normalize Xinit and compute PRIDIT weights, w, using first eigenvector.\n",
    "w = np.sqrt(e_value_1) * e_vector_1\n",
    "norm_factors = np.sqrt(np.diagonal(Xinit.T @ Xinit))\n",
    "Bnorm = Xinit / norm_factors\n",
    "pridit_scores = (Bnorm @ w) / e_value_1\n",
    "\n",
    "# Append scores to original DataFrame, then transform to range [0-100].\n",
    "dfpridit = df.copy(deep=True)\n",
    "dfpridit[\"score_init\"] = pridit_scores.ravel()\n",
    "\n",
    "# Transform scores from [-1, +1] to [0, 100] range.\n",
    "scores_ = dfpridit[\"score_init\"].values\n",
    "dfpridit[\"score\"] = 100 * (\n",
    "    (scores_ - scores_.min()) / (scores_.max() - scores_.min())\n",
    "    )\n",
    "\n",
    "dfpridit[keycols + [\"score\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get variable loadings on first principal component. The variable\n",
    "with the highest PC1_LOADING represents the variable that exhibits\n",
    "the most variation w.r.t. first principal component.\n",
    "\"\"\"\n",
    "dfload = pd.Series(np.abs(e_vector_1.ravel()), index=varcols).to_frame().reset_index(drop=False)\n",
    "dfload.columns = [\"VARIABLE\", \"PC1_LOADING\"]\n",
    "dfload.sort_values(\"PC1_LOADING\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we summarize the distribution of PRIDIT scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate tabular summary of distribution of PRIDIT scores.\n",
    "\"\"\"\n",
    "dfsumm = dfpridit[\"score\"].to_frame().describe()\n",
    "dfsumm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tabular summary of PRIDIT scores at 5 point increments sorted\n",
    "in descending order.\n",
    "\"\"\"\n",
    "lb_ = np.arange(0, 105, 5)\n",
    "ub = np.roll(lb_, -1)[:-1]\n",
    "lb = lb_[:-1]\n",
    "scores_ = dfpridit[\"score\"].values\n",
    "dfthresh = pd.DataFrame({\"lb\":lb, \"ub\":ub})\n",
    "dfthresh[\"RANGE\"] = dfthresh.apply(lambda rec: \"{} <= PRIDIT < {}\".format(rec.lb, rec.ub), axis=1)\n",
    "dfthresh.loc[dfthresh[(dfthresh[\"lb\"]==95) & (dfthresh[\"ub\"]==100)].index, \"range\"] = \"95 <= score <= 100\"\n",
    "dfthresh[\"N\"] = dfthresh.apply(\n",
    "    lambda rec: scores_[np.logical_and(scores_>=rec.lb, scores_<rec.ub)].size, axis=1\n",
    "    )\n",
    "dfthresh.loc[dfthresh[(dfthresh[\"lb\"]==95) & (dfthresh[\"ub\"]==100)].index, \"N\"]+=scores_[scores_==100].size\n",
    "dfthresh.sort_values(\"ub\", ascending=False)[[\"RANGE\", \"N\"]].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize distribution of PRIDIT scores. Exhibit is a histogram\n",
    "with binwidth determined using a modified Freedman-Diaconis rule.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def add_value_labels(ax, spacing=3, annotate_font=6, rotation=0, color=\"#000000\"):\n",
    "    \"\"\"\n",
    "    Insert histogram bin frequency counts above each rectangle in histogram.\n",
    "    \"\"\"\n",
    "    for rect in ax.patches:\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "        label = \"{0:,.0f}\".format(y_value)\n",
    "        ax.annotate(\n",
    "            label, (x_value, y_value), xytext=(0, spacing), textcoords=\"offset points\", \n",
    "            ha=\"center\", va=\"bottom\", fontsize=annotate_font, rotation=rotation,\n",
    "            color=color\n",
    "            )\n",
    "\n",
    "HISTCOLOR = \"#E02C70\"\n",
    "HISTALPHA = .70\n",
    "yvals     = np.sort(dfpridit[\"score\"].values)\n",
    "binwidth  = stats.iqr(yvals, rng=(25, 75), scale=\"raw\", nan_policy=\"omit\") / 6\n",
    "nbrbins   = int(np.ceil((yvals.max() - yvals.min()) / binwidth))\n",
    "titlestr  = \"FWA PRIDIT Score Distribution\"\n",
    "imgpath1  = os.path.join(os.environ[\"TEMP\"], str(uuid.uuid4()) + \".png\")\n",
    "\n",
    "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(9.5, 6.5), dpi=90) \n",
    "\n",
    "ax1.hist(yvals, nbrbins, density=False, alpha=HISTALPHA, color=HISTCOLOR, \n",
    "        edgecolor=\"black\", linewidth=.50)\n",
    "\n",
    "yloc0 = max(rect.get_height() for rect in ax1.patches) * .885\n",
    "\n",
    "xloc0, xloc1, xloc2 = 48, 88, 93\n",
    "\n",
    "ax1.axvline(\n",
    "    50, color=\"#000000\", linestyle=\"--\", linewidth=.9\n",
    "    )\n",
    "ax1.axvline(\n",
    "    90, color=\"#000000\", linestyle=\"--\", linewidth=.9\n",
    "    )\n",
    "ax1.axvline(\n",
    "    95, color=\"#000000\", linestyle=\"--\", linewidth=.9\n",
    "    )\n",
    "ax1.annotate(\n",
    "    \"PRIDIT=50\", (xloc0, yloc0), xytext=(3.5, 0), textcoords=\"offset points\", \n",
    "    ha=\"center\", va=\"bottom\", fontsize=8, rotation=90, color=\"#4b0082\"\n",
    "    )\n",
    "ax1.annotate(\n",
    "    \"PRIDIT=90\", (xloc1, yloc0), xytext=(3.5, 0), textcoords=\"offset points\", \n",
    "    ha=\"center\", va=\"bottom\", fontsize=8, rotation=90, color=\"#4b0082\"\n",
    "    )\n",
    "ax1.annotate(\n",
    "    \"PRIDIT=95\", (xloc2, yloc0), xytext=(3.5, 0), textcoords=\"offset points\", \n",
    "    ha=\"center\", va=\"bottom\", fontsize=8, rotation=90, color=\"#4b0082\"\n",
    "    )\n",
    "\n",
    "add_value_labels(ax=ax1, spacing=2, annotate_font=7, color=\"#576b9b\")\n",
    "ax1.set_title(titlestr, color=\"red\", loc=\"left\", fontsize=9)\n",
    "ax1.get_yaxis().set_major_formatter(mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "ax1.tick_params(axis=\"x\", which=\"major\", labelsize=8)\n",
    "ax1.tick_params(axis=\"x\", which=\"minor\", labelsize=8)\n",
    "ax1.tick_params(axis=\"y\", which=\"major\", labelsize=8)\n",
    "ax1.tick_params(axis=\"y\", which=\"minor\", labelsize=8)\n",
    "ax1.grid(False)\n",
    "plt.savefig(imgpath1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This exhibit is a box-and-whisker plot, which communicates the median \n",
    "and inter-quartile range of PRIDIT scores, and provides some sense of \n",
    "the skewness of the distribution. \n",
    "\"\"\"\n",
    "fig, ax2 = plt.subplots(nrows=1, ncols=1, figsize=(5, 3.5), dpi=90) \n",
    "imgpath2 = os.path.join(os.environ[\"TEMP\"], str(uuid.uuid4()) + \".png\")\n",
    "ax2.boxplot(yvals, notch=True, bootstrap=1000, showfliers=True)\n",
    "ax2.set_title(\"PRIDIT Median + IQR\", color=\"red\", loc=\"left\", fontsize=9)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_xlabel(\"PRIDIT Scores\", fontsize=8)\n",
    "ax2.tick_params(axis=\"y\", which=\"major\", labelsize=8)\n",
    "ax2.tick_params(axis=\"y\", which=\"minor\", labelsize=8)\n",
    "ax2.grid(False)\n",
    "plt.savefig(imgpath2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export PRIDIT score detail, priority claims and PRIDIT transformation \n",
    "summary output.\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Use sklearn in order to bind reference to explaiend variance ratio.\n",
    "pca = PCA(n_components=Z.shape[1])\n",
    "XX = pca.fit_transform(Z)\n",
    "dfvratio = pd.DataFrame(pca.explained_variance_ratio_, columns=[\"explained_variance_ratio\"])\n",
    "dfevecs = pd.DataFrame(e_vectors_)\n",
    "dfevals = pd.DataFrame(e_values_, columns=[\"EIGENVALUES\"])\n",
    "dfscore = dfpridit[keycols + [\"score\"]]\n",
    "dfthresh = dfthresh.sort_values(\"ub\", ascending=False)[[\"RANGE\", \"N\"]].reset_index(drop=True)\n",
    "dfreview = dfpridit.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "dfsumm = dfsumm.reset_index(drop=False).rename({\"index\":\"measure\"}, axis=1)\n",
    "\n",
    "if REVIEW_THRESHOLD.is_integer():\n",
    "    # Keep REVIEW_THRESHOLD records with highest PRIDIT scores.\n",
    "    nbrrecs = REVIEW_THRESHOLD\n",
    "elif REVIEW_THRESHOLD>=0 and REVIEW_THRESHOLD<1:\n",
    "    # Retrain REVIEW_THRESHOLD proportion of records with highest PRIDIT score. \n",
    "    nbrrecs = int(dfpridit.shape[0] * REVIEW_THRESHOLD + 1)\n",
    "else:\n",
    "    raise ValueError(\"REVIEW_THRESHOLD must be numeric and greater than 0.\")\n",
    "    \n",
    "dfreview = dfreview.loc[0:nbrrecs,:]\n",
    "    \n",
    "dexport = [\n",
    "    {\"df\":dfevals,  \"sheetname\":\"Eigenvalues\",         \"header\":True,},\n",
    "    {\"df\":dfevecs,  \"sheetname\":\"Eigenvectors\",        \"header\":True,},\n",
    "    {\"df\":dfcorr,   \"sheetname\":\"Correlation\",         \"header\":True,},\n",
    "    {\"df\":dfload,   \"sheetname\":\"PC1_Loading\",         \"header\":True,},\n",
    "    {\"df\":dfscore,  \"sheetname\":\"PRIDIT_Scores\",       \"header\":True,},\n",
    "    {\"df\":dfsumm,   \"sheetname\":\"PRIDIT_Summary\",      \"header\":False,},\n",
    "    {\"df\":dfthresh, \"sheetname\":\"Scores_by_Threshold\", \"header\":True,},\n",
    "    {\"df\":dfvratio, \"sheetname\":\"Variance_Ratio\",      \"header\":True,},\n",
    "    {\"df\":dfreview, \"sheetname\":\"Priority_Claims\",     \"header\":True,},\n",
    "    ]\n",
    "\n",
    "xlsx_path = os.path.join(EXPORT_PATH, \"FWA_PRIDIT_Summary_{}.xlsx\".format(DATESTAMP))\n",
    "writer = pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\")\n",
    "\n",
    "try:\n",
    "    for d_ in dexport:\n",
    "        try:\n",
    "            print(\"Exporting {}\".format(d_[\"sheetname\"]))\n",
    "            d_[\"df\"].to_excel(\n",
    "                writer, sheet_name=d_[\"sheetname\"], index=False, header=d_[\"header\"]\n",
    "                )\n",
    "        except:\n",
    "            print(\"Error exporting {}\".format(d_[\"sheetname\"]))\n",
    "            \n",
    "    # Embed imgpath1 and imgpath2 in workbook.\n",
    "    pd.DataFrame().to_excel(\n",
    "        writer, sheet_name=\"PRIDIT_Histogram\", startrow=1, header=False, index=False\n",
    "        )\n",
    "    worksheet = writer.sheets[\"PRIDIT_Histogram\"]\n",
    "    worksheet.insert_image(\"A1\", imgpath1)\n",
    "    \n",
    "    pd.DataFrame().to_excel(\n",
    "        writer, sheet_name=\"PRIDIT_Boxplot\", startrow=1, header=False, index=False\n",
    "        )\n",
    "    worksheet = writer.sheets[\"PRIDIT_Boxplot\"]\n",
    "    worksheet.insert_image(\"A1\", imgpath2)\n",
    "    \n",
    "finally:\n",
    "    writer.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
